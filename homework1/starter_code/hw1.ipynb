{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82227481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d836f9",
   "metadata": {},
   "source": [
    "## 2.3 How perplexed can you get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1ed4e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1754f8dae6944fe9143a2bad07279fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "model_id = \"meta-llama/Llama-3.1-8B\"\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model_id, \n",
    "                eos_token_id=128001, pad_token_id=128001, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de08f198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity_for_generated(full_text, prompt, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Calculate per-token and global perplexity for the generated part of text.\n",
    "    \n",
    "    Args:\n",
    "        full_text (str): The complete text (prompt + generated)\n",
    "        prompt (str): The original prompt\n",
    "        model: The language model\n",
    "        tokenizer: The tokenizer\n",
    "    \n",
    "    Returns:\n",
    "        dict: Contains per-token perplexities, global perplexity, and tokens for generated part\n",
    "    \"\"\"\n",
    "    # Tokenize both full text and prompt\n",
    "    full_inputs = tokenizer(full_text, return_tensors=\"pt\")\n",
    "    prompt_inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    full_input_ids = full_inputs.input_ids.to(model.device)\n",
    "    prompt_length = prompt_inputs.input_ids.shape[1]\n",
    "    \n",
    "    # Get model outputs for the full sequence\n",
    "    with torch.no_grad():\n",
    "        outputs = model(full_input_ids)\n",
    "        logits = outputs.logits\n",
    "    \n",
    "    # Calculate log probabilities\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    \n",
    "    # Extract the generated part (everything after the prompt)\n",
    "    generated_token_log_probs = []\n",
    "    generated_tokens = []\n",
    "    \n",
    "    # Start from prompt_length to get only generated tokens\n",
    "    for i in range(prompt_length, full_input_ids.shape[1]):\n",
    "        token_id = full_input_ids[0, i].item()\n",
    "        token_log_prob = log_probs[0, i-1, token_id].item()  # i-1 because logits are shifted\n",
    "        generated_token_log_probs.append(token_log_prob)\n",
    "        generated_tokens.append(tokenizer.decode([token_id]))\n",
    "    \n",
    "    if len(generated_token_log_probs) == 0:\n",
    "        return None  # No generated tokens\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    generated_token_log_probs = np.array(generated_token_log_probs)\n",
    "    \n",
    "    # Calculate per-token perplexity for generated tokens\n",
    "    per_token_perplexity = np.exp(-generated_token_log_probs)\n",
    "    \n",
    "    # Calculate global perplexity for generated tokens\n",
    "    global_perplexity = np.exp(-np.mean(generated_token_log_probs))\n",
    "    \n",
    "    # Extract just the generated text\n",
    "    generated_text = full_text[len(prompt):]\n",
    "    \n",
    "    return {\n",
    "        'generated_tokens': generated_tokens,\n",
    "        'generated_text': generated_text,\n",
    "        'per_token_log_probs': generated_token_log_probs,\n",
    "        'per_token_perplexity': per_token_perplexity,\n",
    "        'global_perplexity': global_perplexity,\n",
    "        'prompt': prompt,\n",
    "        'full_text': full_text\n",
    "    }\n",
    "\n",
    "# Access the model and tokenizer from the pipeline\n",
    "model = pipe.model\n",
    "tokenizer = pipe.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81405d8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The capital of France is one of the most popular tourist destinations in the world, and it’s not hard to see why.'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"The capital of France is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e9b2d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_prompt1 = \"The capital of France is\"\n",
    "low_prompt2 = \"Thank you for your email. I am currently out of the office and will\"\n",
    "low_prompt3 = \"Error 404:\"\n",
    "\n",
    "high_prompt1 = \"In my dream last night, the color blue tasted like\" \n",
    "high_prompt2 = \"How are\"\n",
    "high_prompt3 = \"The last person on Earth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cb47a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GENERATED TEXT PERPLEXITY ANALYSIS ===\n",
      "\n",
      "LOW PERPLEXITY PROMPTS:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr2/mingqia2/miniconda3/envs/llm_env/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr2/mingqia2/miniconda3/envs/llm_env/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Prompt: 'The capital of France is'\n",
      "   Generated: ' a city of many faces. It is a city of history, culture, and art. It is a city of fashion, food, and wine. It is a city of romance, passion, and adventure. It is a city that has something to offer everyone.\n",
      "Paris is a city that is steeped in history.'\n",
      "   Global Perplexity: 1.9427\n",
      "\n",
      "2. Prompt: 'Thank you for your email. I am currently out of the office and will'\n",
      "   Generated: ' be back on 10th April 2019. If your email is urgent, please contact my office on 020 7219 7020.'\n",
      "   Global Perplexity: 2.7863\n",
      "\n",
      "3. Prompt: 'Error 404:'\n",
      "   Generated: ' Page not found. Sorry, but the page you are looking for does not exist. Please check the URL for proper spelling and capitalization. If you're having trouble locating a destination on Penn State Live, please try visiting the sitemap.'\n",
      "   Global Perplexity: 1.6961\n",
      "\n",
      "============================================================\n",
      "HIGH PERPLEXITY PROMPTS:\n",
      "\n",
      "1. Prompt: 'In my dream last night, the color blue tasted like'\n",
      "   Generated: ' a sweet, creamy, vanilla ice cream. I woke up and thought, “I wonder if I could make a blue ice cream?”\n",
      "I did some research and found that blue food coloring is made from a plant called Indigofera tinctoria. It’s a legume that grows in India and is used'\n",
      "   Global Perplexity: 3.3334\n",
      "\n",
      "2. Prompt: 'How are'\n",
      "   Generated: ' you? I hope you are well. I am writing to you to ask for your help. I am a student at the University of the West Indies, Mona Campus. I am currently doing a research project on the topic of the impact of the COVID-19 pandemic on the mental health of Jamaican youth. I am'\n",
      "   Global Perplexity: 2.9947\n",
      "\n",
      "3. Prompt: 'The last person on Earth'\n",
      "   Generated: ' to know about the death of a loved one is the person who died. The first person to know is the person who killed them. The second person to know is the person who found the body. The third person to know is the person who called the police. The fourth person to know is the person who wrote the'\n",
      "   Global Perplexity: 2.0903\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Generate text and calculate perplexity for generated sequences\n",
    "prompts = {\n",
    "    'Low Perplexity': [low_prompt1, low_prompt2, low_prompt3],\n",
    "    'High Perplexity': [high_prompt1, high_prompt2, high_prompt3]\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(\"=== GENERATED TEXT PERPLEXITY ANALYSIS ===\\n\")\n",
    "\n",
    "for category, prompt_list in prompts.items():\n",
    "    print(f\"{category.upper()} PROMPTS:\")\n",
    "    results[category] = []\n",
    "    \n",
    "    for i, prompt in enumerate(prompt_list, 1):\n",
    "        # Generate text\n",
    "        generation = pipe(prompt, max_new_tokens=64, do_sample=False, num_beams=1)\n",
    "        full_text = generation[0]['generated_text']\n",
    "        \n",
    "        # Calculate perplexity for the generated part\n",
    "        result = calculate_perplexity_for_generated(full_text, prompt, model, tokenizer)\n",
    "        \n",
    "        if result is not None:\n",
    "            results[category].append(result)\n",
    "            \n",
    "            print(f\"\\n{i}. Prompt: '{prompt}'\")\n",
    "            print(f\"   Generated: '{result['generated_text']}'\")\n",
    "            print(f\"   Global Perplexity: {result['global_perplexity']:.4f}\")\n",
    "        else:\n",
    "            print(f\"\\n{i}. Prompt: '{prompt}' - No tokens generated\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc0405c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_results = {}\n",
    "for category, res_list in results.items():\n",
    "    formatted_results[category] = []\n",
    "    for res in res_list:\n",
    "        formatted_results[category].append({\n",
    "            'prompt': res['prompt'],\n",
    "            'generated_text': res['generated_text'],\n",
    "            'global_perplexity': res['global_perplexity'],\n",
    "            'per_token_perplexity': res['per_token_perplexity'].tolist(),\n",
    "            'generated_tokens': res['generated_tokens'],\n",
    "            'avg_per_token_perplexity': float(np.mean(res['per_token_perplexity']))\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52c3ee9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"perplexity_results.json\", \"w\") as f:                               \n",
    "    json.dump(formatted_results, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecb22131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LOW PERPLEXITY PROMPTS DETAILED RESULTS:\n",
      "\n",
      "1. Prompt: 'The capital of France is'\n",
      "   Generated: ' a city of many faces. It is a city of history, culture, and art. It is a city of fashion, food, and wine. It is a city of romance, passion, and adventure. It is a city that has something to offer everyone.\n",
      "Paris is a city that is steeped in history.'\n",
      "   Global Perplexity: 1.9427\n",
      "   Average Per-Token Perplexity: 2.4700\n",
      "\n",
      "2. Prompt: 'Thank you for your email. I am currently out of the office and will'\n",
      "   Generated: ' be back on 10th April 2019. If your email is urgent, please contact my office on 020 7219 7020.'\n",
      "   Global Perplexity: 2.7863\n",
      "   Average Per-Token Perplexity: 6.6761\n",
      "\n",
      "3. Prompt: 'Error 404:'\n",
      "   Generated: ' Page not found. Sorry, but the page you are looking for does not exist. Please check the URL for proper spelling and capitalization. If you're having trouble locating a destination on Penn State Live, please try visiting the sitemap.'\n",
      "   Global Perplexity: 1.6961\n",
      "   Average Per-Token Perplexity: 2.6659\n",
      "\n",
      "============================================================\n",
      "\n",
      "HIGH PERPLEXITY PROMPTS DETAILED RESULTS:\n",
      "\n",
      "1. Prompt: 'In my dream last night, the color blue tasted like'\n",
      "   Generated: ' a sweet, creamy, vanilla ice cream. I woke up and thought, “I wonder if I could make a blue ice cream?”\n",
      "I did some research and found that blue food coloring is made from a plant called Indigofera tinctoria. It’s a legume that grows in India and is used'\n",
      "   Global Perplexity: 3.3334\n",
      "   Average Per-Token Perplexity: 4.9496\n",
      "\n",
      "2. Prompt: 'How are'\n",
      "   Generated: ' you? I hope you are well. I am writing to you to ask for your help. I am a student at the University of the West Indies, Mona Campus. I am currently doing a research project on the topic of the impact of the COVID-19 pandemic on the mental health of Jamaican youth. I am'\n",
      "   Global Perplexity: 2.9947\n",
      "   Average Per-Token Perplexity: 4.4556\n",
      "\n",
      "3. Prompt: 'The last person on Earth'\n",
      "   Generated: ' to know about the death of a loved one is the person who died. The first person to know is the person who killed them. The second person to know is the person who found the body. The third person to know is the person who called the police. The fourth person to know is the person who wrote the'\n",
      "   Global Perplexity: 2.0903\n",
      "   Average Per-Token Perplexity: 3.2293\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "for category, res_list in formatted_results.items():\n",
    "    print(f\"\\n{category.upper()} PROMPTS DETAILED RESULTS:\")\n",
    "    for i, res in enumerate(res_list, 1):\n",
    "        print(f\"\\n{i}. Prompt: '{res['prompt']}'\")\n",
    "        print(f\"   Generated: '{res['generated_text']}'\")\n",
    "        print(f\"   Global Perplexity: {res['global_perplexity']:.4f}\")\n",
    "        print(f\"   Average Per-Token Perplexity: {res['avg_per_token_perplexity']:.4f}\")\n",
    "        # print(f\"   Per-Token Perplexities: {res['per_token_perplexity']}\")\n",
    "        # print(f\"   Generated Tokens: {res['generated_tokens']}\")\n",
    "    print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ef4ccf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'Thank you for your email. I am currently out of the office and will',\n",
       " 'generated_text': ' be back on 10th April 2019. If your email is urgent, please contact my office on 020 7219 7020.',\n",
       " 'global_perplexity': 2.786281929260581,\n",
       " 'per_token_perplexity': [3.849644694489727,\n",
       "  2.8508591804911467,\n",
       "  2.195663156426126,\n",
       "  3.4060082199050283,\n",
       "  27.041567040558693,\n",
       "  3.1996512838248377,\n",
       "  9.418717264809843,\n",
       "  2.8625995797040673,\n",
       "  1.1216551092306364,\n",
       "  1.7614440859161942,\n",
       "  2.0104832294895387,\n",
       "  2.3807405945617703,\n",
       "  2.0220501727307085,\n",
       "  3.2358653201368193,\n",
       "  1.2105000000687076,\n",
       "  1.5542683678611078,\n",
       "  1.8838506266306723,\n",
       "  1.0841472151303213,\n",
       "  1.5837806209268046,\n",
       "  9.04368226357122,\n",
       "  5.073446339505259,\n",
       "  1.5771913219565126,\n",
       "  1.179865126698548,\n",
       "  2.048208231059669,\n",
       "  1.692588484969701,\n",
       "  2.502174797210619,\n",
       "  1.039388896054288,\n",
       "  1.0044697524242978,\n",
       "  99.9017308888125,\n",
       "  4.959717963651696,\n",
       "  2.2619619633674506],\n",
       " 'generated_tokens': [' be',\n",
       "  ' back',\n",
       "  ' on',\n",
       "  ' ',\n",
       "  '10',\n",
       "  'th',\n",
       "  ' April',\n",
       "  ' ',\n",
       "  '201',\n",
       "  '9',\n",
       "  '.',\n",
       "  ' If',\n",
       "  ' your',\n",
       "  ' email',\n",
       "  ' is',\n",
       "  ' urgent',\n",
       "  ',',\n",
       "  ' please',\n",
       "  ' contact',\n",
       "  ' my',\n",
       "  ' office',\n",
       "  ' on',\n",
       "  ' ',\n",
       "  '020',\n",
       "  ' ',\n",
       "  '721',\n",
       "  '9',\n",
       "  ' ',\n",
       "  '702',\n",
       "  '0',\n",
       "  '.'],\n",
       " 'avg_per_token_perplexity': 6.676061993295951}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_results['Low Perplexity'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f058c2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d5c2e99",
   "metadata": {},
   "source": [
    "## 2.4 Beam Search Puzzle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8708225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b35471863dad493db45be59b11b9f994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen3-1.7B\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3c5a489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[71, 64634, 13788, 151645, 151643, 151643, 151643, 151643, 151643]\n",
      "hunanexpress\n",
      "[71, 64634, 13788, 151668, 271, 71, 64634, 13788, 151645]\n",
      "hunanexpress\n",
      "[71, 359, 309, 327, 1726, 288, 151645, 151643, 151643]\n",
      "hunamexprees\n",
      "[71, 359, 309, 327, 1726, 325, 151645, 151643, 151643]\n",
      "hunamexprese\n",
      "[71, 359, 309, 327, 1726, 778, 151645, 151643, 151643]\n",
      "hunamexpress\n",
      "[71, 359, 309, 327, 1726, 325, 417, 151645, 151643]\n",
      "hunamexpresept\n",
      "[71, 359, 276, 13788, 151645, 151643, 151643, 151643, 151643]\n",
      "hunanexpress\n",
      "[71, 359, 309, 327, 1726, 325, 267, 151645, 151643]\n",
      "hunamexpresest\n",
      "[71, 359, 309, 327, 1873, 151645, 151643, 151643, 151643]\n",
      "hunamexpress\n",
      "[71, 359, 309, 327, 649, 778, 151645, 151643, 151643]\n",
      "hunamexprss\n"
     ]
    }
   ],
   "source": [
    "# prepare the model input\n",
    "# prompt = \"Print the string ‘break’ and string 'fast' without white space! DON'T GENERATE ANYTHING ELSE\"\n",
    "prompt = \"give me the string of: 'hunanexpress'. DON'T RETURN ANYTHING ELSE.\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False \n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# conduct text completion\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512,\n",
    "    num_beams=10,\n",
    "    num_return_sequences=10,\n",
    "    early_stopping=False\n",
    ")\n",
    "\n",
    "for out in generated_ids:\n",
    "    output_ids = out[len(model_inputs.input_ids[0]):].tolist()\n",
    "    print(output_ids)\n",
    "    try:\n",
    "        index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "    except ValueError:\n",
    "        index = 0\n",
    "    print(tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad0dfa4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'an'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_token = tokenizer.convert_ids_to_tokens(276)\n",
    "decoded_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a62054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'un'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_token = tokenizer.convert_ids_to_tokens(359)\n",
    "decoded_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "efe0dd87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'unan'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_token = tokenizer.convert_ids_to_tokens(64634)\n",
    "decoded_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33fbbbcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ġeat'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_token = tokenizer.convert_ids_to_tokens(8180)\n",
    "decoded_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03081359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.Ċ'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_token = tokenizer.convert_ids_to_tokens(624)\n",
    "decoded_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acdb436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa74700",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1123b03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

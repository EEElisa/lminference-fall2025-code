{
  "batch_id": 0,
  "batch_size": 3,
  "scheduled_arrival_time": 8.230951251669854,
  "actual_send_time": 8.236440896987915,
  "request_duration": 86.33942937850952,
  "completion_time": 94.57590627670288,
  "status_code": 200,
  "prompt_idxs": [
    2,
    0,
    3
  ],
  "response": {
    "error": "Already borrowed",
    "traceback": "Traceback (most recent call last):\n  File \"/root/system_with_router.py\", line 683, in completions\n    task_type, task_indices, generated_texts, prompt_lengths, outputs = future.result()\n                                                                        ^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/usr/local/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/system_with_router.py\", line 661, in _run_group\n    batch_results = [\n                    ^\n  File \"/root/system_with_router.py\", line 662, in <listcomp>\n    self._enqueue_for_batch(task_type, p, max_tokens, temperature)\n  File \"/root/system_with_router.py\", line 256, in _enqueue_for_batch\n    raise entry[\"result\"]\n  File \"/root/system_with_router.py\", line 221, in _batch_worker\n    generated_texts, prompt_lengths, outputs = self._process_by_task(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/system_with_router.py\", line 567, in _process_by_task\n    gen_ids, acc_rate, prompt_len, prompt_ids = specdec.generate_one(\n                                                ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/specdec.py\", line 232, in generate_one\n    tokenized = self.tokenizer(\n                ^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py\", line 3073, in __call__\n    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py\", line 3183, in _call_one\n    return self.encode_plus(\n           ^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py\", line 3258, in encode_plus\n    return self._encode_plus(\n           ^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py\", line 627, in _encode_plus\n    batched_output = self._batch_encode_plus(\n                     ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py\", line 541, in _batch_encode_plus\n    self.set_truncation_and_padding(\n  File \"/usr/local/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py\", line 498, in set_truncation_and_padding\n    self._tokenizer.no_padding()\nRuntimeError: Already borrowed\n"
  },
  "error": null
}